{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -U transformers datasets evaluate seqeval"
      ],
      "metadata": {
        "id": "zMhxH-TDb-RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "CoInK8r5cDPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers[torch]"
      ],
      "metadata": {
        "id": "AcRXkWADcEPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "W5ba_j-mcFVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "al54NLNHcGQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_json('data.json')"
      ],
      "metadata": {
        "id": "LlgmqId9cHCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelList = []\n",
        "for i in range(len(ds)):\n",
        "  labelList.append(ds[i]['annotations'][0]['label'])"
      ],
      "metadata": {
        "id": "P2ME1equuQ-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6dvX-foKuTKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "dataArray = np.array(ds)\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(ds, labelList):\n",
        "    strat_train_set = list(dataArray[train_index])\n",
        "    strat_test_set = list(dataArray[test_index])"
      ],
      "metadata": {
        "id": "qBlkbFPOuUMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nerdataset = datasets.DatasetDict({'train': strat_train_set}) #switch strat_train_set and strat_test_set for following steps to get train set and test set"
      ],
      "metadata": {
        "id": "NGz-7CfPcIQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_listner = ['0','DOMAIN_NAME','GEO_LOCATION','URL','MALWARE_NAME','HASH_VAL','ACTOR','FILE_PATH','IP','REG_KEY','EMAIL_ADDRESSES']"
      ],
      "metadata": {
        "id": "iXWsHgJdcJn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_listnerI = ['0','DOMAIN_NAME','GEO_LOCATION','URL','MALWARE_NAME','HASH_VAL','ACTOR','FILE_PATH','IP','REG_KEY','EMAIL_ADDRESSES']"
      ],
      "metadata": {
        "id": "JZfnZqvVcK1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "FaP4Z3jg5_p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "procesData = []\n",
        "bull = ['a','an','the','and','on','in','at','of','I']\n",
        "for i in range(len(nerdataset[\"train\"])):\n",
        "  texttokens = re.findall(r\"[\\w']+|[.,!?;]\", nerdataset[\"train\"][i]['text'])\n",
        "  dic = {\"text\": texttokens,\"annotations\":[0] * len(texttokens)}\n",
        "\n",
        "  entityL = []\n",
        "  for y in range(len(nerdataset[\"train\"][i]['annotations'])):\n",
        "    entityL.append(nerdataset[\"train\"][i]['annotations'][y]['entity'])\n",
        "\n",
        "  for f in range(len(texttokens)):\n",
        "    if f == 0:#在句首用B\n",
        "      for k in range(len(entityL)):\n",
        "        if texttokens[f] in entityL[k]:\n",
        "          if texttokens[f] not in bull:\n",
        "            index = label_listner.index(nerdataset[\"train\"][i]['annotations'][k]['label'])\n",
        "            dic['annotations'][f] = index\n",
        "    else:\n",
        "      if dic['annotations'][f-1] != 0:#上一个tag不等于0用I\n",
        "        for k in range(len(entityL)):\n",
        "          if texttokens[f] in entityL[k]:\n",
        "            if texttokens[f] not in bull:\n",
        "              index = label_listnerI.index(nerdataset[\"train\"][i]['annotations'][k]['label'])\n",
        "              index = index + len(label_listnerI) - 1\n",
        "              dic['annotations'][f] = index\n",
        "      else:#上一个tag等于0用B\n",
        "        for k in range(len(entityL)):\n",
        "          if texttokens[f] in entityL[k]:\n",
        "            if texttokens[f] not in bull:\n",
        "              index = label_listner.index(nerdataset[\"train\"][i]['annotations'][k]['label'])\n",
        "              dic['annotations'][f] = index\n",
        "\n",
        "  procesData.append(dic)"
      ],
      "metadata": {
        "id": "dbsuYOcKC4-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "DR21zr_wcNro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dumps(procesData)"
      ],
      "metadata": {
        "id": "eg_1pOGjcOge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('nerdata.json', 'w')\n",
        "file.write(json.dumps(procesData))\n",
        "file.close()"
      ],
      "metadata": {
        "id": "x7BYuexYXxwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pds = Dataset.from_json('nerdata.json')"
      ],
      "metadata": {
        "id": "F1AQNMilcQgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdst = Dataset.from_json('nerdataTest.json')"
      ],
      "metadata": {
        "id": "WYp0YrYCvzj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pnerdataset = datasets.DatasetDict({'train': pds})"
      ],
      "metadata": {
        "id": "BIyweoR5cSJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pnerdatasetTest = datasets.DatasetDict({'train': pdst})"
      ],
      "metadata": {
        "id": "8JboQ3tjv4HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "0WFHbzqNcTjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ],
      "metadata": {
        "id": "k_ex8XFwt_H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"annotations\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "hVHnolcScTeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_pnerdatasetTrain = pnerdataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "eBzHcmQJcWkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_pnerdatasetTest = pnerdatasetTest.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "a5Hvzcl2wfUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "rbRo-YIDcYk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "MDwQF_mccZ4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_listnerBI = [\"O\",\"B-DOMAIN_NAME\",\"B-GEO_LOCATION\",\"B-URL\",\"B-MALWARE_NAME\",\"B-HASH_VAL\",\"B-ACTOR\",\"B-FILE_PATH\",\"B-IP\",\"B-REG_KEY\",\"B-EMAIL_ADDRESSES\",\"I-DOMAIN_NAME\",\"I-GEO_LOCATION\",\"I-URL\",\"I-MALWARE_NAME\",\"I-HASH_VAL\",\"I-ACTOR\",\"I-FILE_PATH\",\"I-IP\",\"I-REG_KEY\",\"I-EMAIL_ADDRESSES\"]"
      ],
      "metadata": {
        "id": "o1mPNKZNcbVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_listnerBI[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_listnerBI[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "LB-96zZNccU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2labelner = {\n",
        "    0: \"O\",\n",
        "    1: \"B-DOMAIN_NAME\",\n",
        "    2: \"B-GEO_LOCATION\",\n",
        "    3: \"B-URL\",\n",
        "    4: \"B-MALWARE_NAME\",\n",
        "    5: \"B-HASH_VAL\",\n",
        "    6: \"B-ACTOR\",\n",
        "    7: \"B-FILE_PATH\",\n",
        "    8: \"B-IP\",\n",
        "    9: \"B-REG_KEY\",\n",
        "    10: \"B-EMAIL_ADDRESSES\",\n",
        "    11: \"I-DOMAIN_NAME\",\n",
        "    12: \"I-GEO_LOCATION\",\n",
        "    13: \"I-URL\",\n",
        "    14: \"I-MALWARE_NAME\",\n",
        "    15: \"I-HASH_VAL\",\n",
        "    16: \"I-ACTOR\",\n",
        "    17: \"I-FILE_PATH\",\n",
        "    18: \"I-IP\",\n",
        "    19: \"I-REG_KEY\",\n",
        "    20: \"I-EMAIL_ADDRESSES\",\n",
        "}\n",
        "label2idner = {\n",
        "    \"O\": 0,\n",
        "    \"B-DOMAIN_NAME\": 1,\n",
        "    \"B-GEO_LOCATION\": 2,\n",
        "    \"B-URL\": 3,\n",
        "    \"B-MALWARE_NAME\": 4,\n",
        "    \"B-HASH_VAL\": 5,\n",
        "    \"B-ACTOR\": 6,\n",
        "    \"B-FILE_PATH\": 7,\n",
        "    \"B-IP\": 8,\n",
        "    \"B-REG_KEY\": 9,\n",
        "    \"B-EMAIL_ADDRESSES\": 10,\n",
        "    \"I-DOMAIN_NAME\": 11,\n",
        "    \"I-GEO_LOCATION\": 12,\n",
        "    \"I-URL\": 13,\n",
        "    \"I-MALWARE_NAME\": 14,\n",
        "    \"I-HASH_VAL\": 15,\n",
        "    \"I-ACTOR\": 16,\n",
        "    \"I-FILE_PATH\": 17,\n",
        "    \"I-IP\": 18,\n",
        "    \"I-REG_KEY\": 19,\n",
        "    \"I-EMAIL_ADDRESSES\": 20,\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "7vbbWVPLcd-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=21, id2label=id2labelner, label2id=label2idner\n",
        ")"
      ],
      "metadata": {
        "id": "DDWqy_BschOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/trainModel\",\n",
        "    learning_rate=0.0001,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=50,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_pnerdatasetTrain[\"train\"],\n",
        "    eval_dataset=tokenized_pnerdatasetTest[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eBaJ3hV5ciXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "JRtYe6AgckAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"seqeval\")"
      ],
      "metadata": {
        "id": "TE9zXh1NcmDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, labels, _ = trainer.predict(tokenized_pnerdatasetTest[\"train\"])\n",
        "predictions1 = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_listnerBI[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions1, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [label_listnerBI[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions1, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "id": "URUh_84ccnTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}