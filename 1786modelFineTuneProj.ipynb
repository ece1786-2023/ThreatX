{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -U transformers datasets evaluate seqeval"
      ],
      "metadata": {
        "id": "zMhxH-TDb-RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "CoInK8r5cDPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers[torch]"
      ],
      "metadata": {
        "id": "AcRXkWADcEPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "W5ba_j-mcFVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "al54NLNHcGQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = Dataset.from_json('data.json')"
      ],
      "metadata": {
        "id": "LlgmqId9cHCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nerdataset = datasets.DatasetDict({'train': ds})"
      ],
      "metadata": {
        "id": "NGz-7CfPcIQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_listner = ['0','DOMAIN_NAME','GEO_LOCATION','URL','MALWARE_NAME','HASH_VAL','ACTOR','FILE_PATH','IP','REG_KEY','EMAIL_ADDRESSES']"
      ],
      "metadata": {
        "id": "iXWsHgJdcJn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_listnerI = ['0','DOMAIN_NAME','GEO_LOCATION','URL','MALWARE_NAME','HASH_VAL','ACTOR','FILE_PATH','IP','REG_KEY','EMAIL_ADDRESSES']"
      ],
      "metadata": {
        "id": "JZfnZqvVcK1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "procesData = []\n",
        "for i in range(len(nerdataset[\"train\"])):\n",
        "  ctext = nerdataset[\"train\"][i]['text']\n",
        "  texttokens = ctext.split()\n",
        "  dic = {\"text\": texttokens,\"annotations\":[]}\n",
        "\n",
        "  count = 0\n",
        "  for f in range(len(texttokens)):\n",
        "    if f == 0:#在句首用B\n",
        "      if count == len(nerdataset[\"train\"][i]['annotations']):\n",
        "        dic['annotations'].append(0)\n",
        "      else:\n",
        "        if texttokens[f] == nerdataset[\"train\"][i]['annotations'][count]['entity']:\n",
        "          index = label_listner.index(nerdataset[\"train\"][i]['annotations'][count]['label'])\n",
        "          dic['annotations'].append(index)\n",
        "          count = count + 1\n",
        "        else:\n",
        "          dic['annotations'].append(0)\n",
        "    else:\n",
        "      if dic['annotations'][-1] != 0:#上一个tag不等于0用I\n",
        "        if count == len(nerdataset[\"train\"][i]['annotations']):\n",
        "          dic['annotations'].append(0)\n",
        "        else:\n",
        "          if texttokens[f] == nerdataset[\"train\"][i]['annotations'][count]['entity']:\n",
        "            index = label_listnerI.index(nerdataset[\"train\"][i]['annotations'][count]['label'])\n",
        "            index = index + len(label_listnerI)\n",
        "            dic['annotations'].append(index)\n",
        "            count = count + 1\n",
        "          else:\n",
        "            dic['annotations'].append(0)\n",
        "      else:#上一个tag等于0用B\n",
        "        if count == len(nerdataset[\"train\"][i]['annotations']):\n",
        "          dic['annotations'].append(0)\n",
        "        else:\n",
        "          if texttokens[f] == nerdataset[\"train\"][i]['annotations'][count]['entity']:\n",
        "            index = label_listner.index(nerdataset[\"train\"][i]['annotations'][count]['label'])\n",
        "            dic['annotations'].append(index)\n",
        "            count = count + 1\n",
        "          else:\n",
        "            dic['annotations'].append(0)\n",
        "\n",
        "  procesData.append(dic)"
      ],
      "metadata": {
        "id": "xUkRILXKcLv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "DR21zr_wcNro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.dumps(procesData)"
      ],
      "metadata": {
        "id": "eg_1pOGjcOge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pds = Dataset.from_json('nerdata.json')"
      ],
      "metadata": {
        "id": "F1AQNMilcQgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pnerdataset = datasets.DatasetDict({'train': pds})"
      ],
      "metadata": {
        "id": "BIyweoR5cSJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "0WFHbzqNcTjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"annotations\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "hVHnolcScTeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_pnerdataset = pnerdataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "eBzHcmQJcWkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "rbRo-YIDcYk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "MDwQF_mccZ4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_listnerBI = [\"O\",\"B-DOMAIN_NAME\",\"B-GEO_LOCATION\",\"B-URL\",\"B-MALWARE_NAME\",\"B-HASH_VAL\",\"B-ACTOR\",\"B-FILE_PATH\",\"B-IP\",\"B-REG_KEY\",\"B-EMAIL_ADDRESSES\",\"I-DOMAIN_NAME\",\"I-GEO_LOCATION\",\"I-URL\",\"I-MALWARE_NAME\",\"I-HASH_VAL\",\"I-ACTOR\",\"I-FILE_PATH\",\"I-IP\",\"I-REG_KEY\",\"I-EMAIL_ADDRESSES\"]"
      ],
      "metadata": {
        "id": "o1mPNKZNcbVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_listnerBI[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_listnerBI[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "LB-96zZNccU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2labelner = {\n",
        "    0: \"O\",\n",
        "    1: \"B-DOMAIN_NAME\",\n",
        "    2: \"B-GEO_LOCATION\",\n",
        "    3: \"B-URL\",\n",
        "    4: \"B-MALWARE_NAME\",\n",
        "    5: \"B-HASH_VAL\",\n",
        "    6: \"B-ACTOR\",\n",
        "    7: \"B-FILE_PATH\",\n",
        "    8: \"B-IP\",\n",
        "    9: \"B-REG_KEY\",\n",
        "    10: \"B-EMAIL_ADDRESSES\",\n",
        "    11: \"I-DOMAIN_NAME\",\n",
        "    12: \"I-GEO_LOCATION\",\n",
        "    13: \"I-URL\",\n",
        "    14: \"I-MALWARE_NAME\",\n",
        "    15: \"I-HASH_VAL\",\n",
        "    16: \"I-ACTOR\",\n",
        "    17: \"I-FILE_PATH\",\n",
        "    18: \"I-IP\",\n",
        "    19: \"I-REG_KEY\",\n",
        "    20: \"I-EMAIL_ADDRESSES\",\n",
        "}\n",
        "label2idner = {\n",
        "    \"O\": 0,\n",
        "    \"B-DOMAIN_NAME\": 1,\n",
        "    \"B-GEO_LOCATION\": 2,\n",
        "    \"B-URL\": 3,\n",
        "    \"B-MALWARE_NAME\": 4,\n",
        "    \"B-HASH_VAL\": 5,\n",
        "    \"B-ACTOR\": 6,\n",
        "    \"B-FILE_PATH\": 7,\n",
        "    \"B-IP\": 8,\n",
        "    \"B-REG_KEY\": 9,\n",
        "    \"B-EMAIL_ADDRESSES\": 10,\n",
        "    \"I-DOMAIN_NAME\": 11,\n",
        "    \"I-GEO_LOCATION\": 12,\n",
        "    \"I-URL\": 13,\n",
        "    \"I-MALWARE_NAME\": 14,\n",
        "    \"I-HASH_VAL\": 15,\n",
        "    \"I-ACTOR\": 16,\n",
        "    \"I-FILE_PATH\": 17,\n",
        "    \"I-IP\": 18,\n",
        "    \"I-REG_KEY\": 19,\n",
        "    \"I-EMAIL_ADDRESSES\": 20,\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "7vbbWVPLcd-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_datasetT = tokenized_pnerdataset[\"train\"].shuffle(seed=42).select(range(64))\n",
        "small_eval_datasetT = tokenized_pnerdataset[\"train\"].shuffle(seed=42).select(range(16))"
      ],
      "metadata": {
        "id": "a5P_dqvgcf3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=21, id2label=id2labelner, label2id=label2idner\n",
        ")"
      ],
      "metadata": {
        "id": "DDWqy_BschOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/trainModel\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_datasetT,\n",
        "    eval_dataset=small_eval_datasetT,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eBaJ3hV5ciXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "JRtYe6AgckAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"seqeval\")"
      ],
      "metadata": {
        "id": "TE9zXh1NcmDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, labels, _ = trainer.predict(tokenized_pnerdataset[\"train\"])\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [id2labelner[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "true_labels = [\n",
        "    [id2labelner[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "results"
      ],
      "metadata": {
        "id": "URUh_84ccnTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}